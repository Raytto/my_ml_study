{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04550c3-0a76-47d3-b3ce-fa87c9edf717",
   "metadata": {},
   "source": [
    "# 了解 Self-attention 的基本数学思路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6add369-72de-4fbf-9e80-6f80b7170e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30299289-2054-4ae8-b3e1-c6e1ae72c9ba",
   "metadata": {},
   "source": [
    "## 1. 生成几批序列数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca46df1-55f2-4032-8956-513e636e5802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个3维张量作为输入\n",
    "# batch, time, channels。批次，token位置，token向量各维度参数\n",
    "B, T, C = 2, 8, 2\n",
    "x = torch.rand(B, T, C)\n",
    "# 查看x形状\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3d1f22-5dcc-476a-b5ee-c4f66a0a29f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0687, 0.4276],\n",
       "         [0.0396, 0.3199],\n",
       "         [0.4048, 0.4472],\n",
       "         [0.1638, 0.5660],\n",
       "         [0.6666, 0.0691],\n",
       "         [0.5223, 0.1797],\n",
       "         [0.1815, 0.9351],\n",
       "         [0.8868, 0.3212]],\n",
       "\n",
       "        [[0.1570, 0.9270],\n",
       "         [0.8815, 0.3526],\n",
       "         [0.0064, 0.6480],\n",
       "         [0.6853, 0.6159],\n",
       "         [0.8965, 0.9815],\n",
       "         [0.2357, 0.9653],\n",
       "         [0.3571, 0.7171],\n",
       "         [0.8771, 0.6371]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看x内容，把这看做两批数据(B),每批有8个token(T)，每个token用2个参数描述(C)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ac544-9dab-4885-9b97-8a5767963f2d",
   "metadata": {},
   "source": [
    "## 2. 考虑使token注意到前面的token\n",
    "* 我们希望这8个token能够相互交流（注意 attention 到）\n",
    "* 同时我们希望每个 token 仅注意到它之前的 token (以更像在生成)，如第5个token仅能注意到前4个token\n",
    "* 最简单的让 token 获取前方信息的方式就是把前面全部取平均，得到一个向量去描述前面信息（平均通常没什么用，且丢失了前方的token的位置信息，但用于示例很方便）。未来再考虑用其他的方式获取前面信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38066610-11c5-4146-9cf9-50bc37906bd1",
   "metadata": {},
   "source": [
    "### 2.1 先考虑最简单粗暴的取前面平均值的方式：循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e6d828-99ab-48bd-96c1-175b14994ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.4276],\n",
       "        [0.0396, 0.3199]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于张量可以用这种方式抽取其中一部分\n",
    "x[0, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6ab5b3-b212-4451-b82c-125f06870e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow means bag of words，表征抽取前面的信息\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]  # (t,C)\n",
    "        # 对到自己的位置的值取平均\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305d8613-5e2b-4aca-b211-7f0f10e81695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.4276],\n",
       "        [0.0396, 0.3199],\n",
       "        [0.4048, 0.4472],\n",
       "        [0.1638, 0.5660],\n",
       "        [0.6666, 0.0691],\n",
       "        [0.5223, 0.1797],\n",
       "        [0.1815, 0.9351],\n",
       "        [0.8868, 0.3212]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始输入\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6324cbe-e3e6-4ec2-a60e-e93d1939b207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.4276],\n",
       "        [0.0541, 0.3738],\n",
       "        [0.1710, 0.3982],\n",
       "        [0.1692, 0.4402],\n",
       "        [0.2687, 0.3660],\n",
       "        [0.3110, 0.3349],\n",
       "        [0.2925, 0.4207],\n",
       "        [0.3667, 0.4082]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对前面取平均后的结果\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac966e-be16-45ae-9ea2-58cf32836abd",
   "metadata": {},
   "source": [
    "### 2.2 换用矩阵乘法取平均的方式\n",
    "循环的方式比较直观，但低效，且不方便推广。 \n",
    "而矩阵是更方便的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea406018-cb48-4eb8-8099-d62af1913306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于只对前面取平均，可以用 下三角矩阵。0位置自然就会在乘法后被忽略掉\n",
    "a = torch.tril(torch.ones(4, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80fb5aff-8755-4c07-a4ae-ae23ec119065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用这样的矩阵乘一个列向量等效在对列向量进行加权求和了（每行为一系列权重）。但由于我们是在取平均值，所以需要让每一行的和归一化\n",
    "# 先按行求和\n",
    "a_sum_by_1 = torch.sum(a, 1, keepdim=True)\n",
    "a_sum_by_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e0c68f7-e50d-4624-aa13-6561be37adb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按行除\n",
    "a = a / a_sum_by_1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "214f3b35-e0bc-4fd1-8086-b465240eef67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于上面的思路再求前面平均值的 xbow2\n",
    "# 先得到权重矩阵\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b8ec64-3261-4bdc-b385-6b0bdf8feb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0687, 0.4276],\n",
       "         [0.0541, 0.3738],\n",
       "         [0.1710, 0.3982],\n",
       "         [0.1692, 0.4402],\n",
       "         [0.2687, 0.3660],\n",
       "         [0.3110, 0.3349],\n",
       "         [0.2925, 0.4207],\n",
       "         [0.3667, 0.4082]],\n",
       "\n",
       "        [[0.1570, 0.9270],\n",
       "         [0.5193, 0.6398],\n",
       "         [0.3483, 0.6425],\n",
       "         [0.4326, 0.6359],\n",
       "         [0.5254, 0.7050],\n",
       "         [0.4771, 0.7484],\n",
       "         [0.4599, 0.7439],\n",
       "         [0.5121, 0.7306]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再用这个矩阵乘 x 得到 xbow2\n",
    "# (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
    "xbow2 = wei @ x\n",
    "xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15359497-881b-4e2a-b4bc-b13132248bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对比两种方式计算的xbow，应该是一样的\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8a499-98c9-4831-ab39-6b5dc9703890",
   "metadata": {},
   "source": [
    "### 2.3 进一步我们利用 softmax 函数\n",
    "softmax 可以把一堆实数映射到 [0,1] 区间，并保证映射后的和为1，非常适合用于计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5395a938-83d8-466a-893c-8f843793daeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "# softmax 会把 -inf 映射为0，所以需要把希望为0的地方置为 -inf\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "# 对各个最后一维，即行，进行softmax\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b247d1d-766e-48c0-9e81-d074b0691d49",
   "metadata": {},
   "source": [
    "可见可以得到和前面用sum除一样的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ebb2142-48e1-4810-a9e4-56e14c863c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0687, 0.4276],\n",
       "         [0.0541, 0.3738],\n",
       "         [0.1710, 0.3982],\n",
       "         [0.1692, 0.4402],\n",
       "         [0.2687, 0.3660],\n",
       "         [0.3110, 0.3349],\n",
       "         [0.2925, 0.4207],\n",
       "         [0.3667, 0.4082]],\n",
       "\n",
       "        [[0.1570, 0.9270],\n",
       "         [0.5193, 0.6398],\n",
       "         [0.3483, 0.6425],\n",
       "         [0.4326, 0.6359],\n",
       "         [0.5254, 0.7050],\n",
       "         [0.4771, 0.7484],\n",
       "         [0.4599, 0.7439],\n",
       "         [0.5121, 0.7306]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样方式计算xbow3\n",
    "xbow3 = wei @ x\n",
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa862788-fa89-4223-846b-f0536071af68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xbow3 也能和前两种方式结果一致\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a489992-9c5d-412f-89a8-e12114014e28",
   "metadata": {},
   "source": [
    "## 3. 用self-attention的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af415a3b-c38d-4ac7-814e-da69c315dcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 扩展一下输入数据的大小，重新设置输入数据\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.rand(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b2425-081e-4c2b-b79a-4df56efa3954",
   "metadata": {},
   "source": [
    "### 3.1 实现不带v的单头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b3be002-78de-4fe1-b6ce-26f6fd51e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 注意头的大小设置\n",
    "head_size = 16\n",
    "# 每个头需要一个 key 矩阵，作用到x上以后提取x的特征信息\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# 每个头需要一个 query 矩阵，作用到x上以后提取想问的问题\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# 作用于x得到具体的key和query\n",
    "k = key(x)  # (B,T,16)\n",
    "q = query(x)  # (B,T,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d7cfb6d-aad2-4d6c-99be-6deb487d0800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d4392f5-4956-4b21-aa98-5f893b6afd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4314ed1-83f0-4862-b046-a6b1f52246af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 让 k和q进行内积，获得key和query到底有多么匹配\n",
    "# 为了求内积，需要转置一下后两维\n",
    "# (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "# 注：忽略B以后，k 和 q 都是由T个行向量组成的矩阵，内积有T*T组，对应T*T结果\n",
    "# 这个 wei 类似之前的取平均的注意力矩阵类似，表征每个token应该有多么关心其他各个token\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33f1d82a-7d83-471a-9124-678cc06ff962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5484,  0.0883,  0.3871, -0.2779,  0.0643,  0.5207,  0.2659, -0.4657],\n",
       "        [-0.0583,  0.1873,  0.5663,  0.3572,  0.0514,  0.7701,  0.9311, -0.1547],\n",
       "        [-0.8897, -0.4039,  0.0098, -0.5331, -0.2614,  0.1063, -0.1382, -0.8162],\n",
       "        [-0.7110, -0.3480,  0.0835, -0.5274, -0.6238,  0.0388,  0.0916, -0.9112],\n",
       "        [-0.5468, -0.1322,  0.3889, -0.2240, -0.1973,  0.3237,  0.3277, -0.5813],\n",
       "        [-0.5489, -0.2345,  0.2170, -0.3353, -0.2469,  0.0510,  0.1611, -0.5779],\n",
       "        [-0.7004, -0.4023,  0.1859, -0.4054, -0.4893,  0.3774,  0.3124, -0.7932],\n",
       "        [-0.8228, -0.2948, -0.1135, -0.5053, -0.4677,  0.0635, -0.0205, -0.7400]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看第一个batch的wei的权重矩阵\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "029da6fd-65fd-4865-a5f6-f624b9aa3f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4389, 0.5611, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1967, 0.3197, 0.4836, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1709, 0.2456, 0.3782, 0.2053, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1272, 0.1925, 0.3242, 0.1757, 0.1804, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1119, 0.1533, 0.2408, 0.1386, 0.1514, 0.2039, 0.0000, 0.0000],\n",
       "        [0.0767, 0.1033, 0.1860, 0.1030, 0.0947, 0.2253, 0.2111, 0.0000],\n",
       "        [0.0753, 0.1278, 0.1531, 0.1035, 0.1075, 0.1828, 0.1681, 0.0819]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类似之前2.3的做法，我们让各个token仅关注其前面的token，并且用softmax归一化\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4bedd32-bc4c-4856-adcc-e695aa5b7108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算加权平均\n",
    "out = wei @ x\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12787ca-e8b6-42ff-aa40-a900613ed4be",
   "metadata": {},
   "source": [
    "### 3.2 实现带v的单头注意力机制\n",
    "刚才的做法下，仅基于key 和 query 的内积  \n",
    "self-attention 还进一步对 x 进行了一次变换，用另一个矩阵v  \n",
    "\n",
    "合在一起的思路是：\n",
    "* query 是我想问我自己的一批问题\n",
    "* key 是我基于我自己的信息的一批特征\n",
    "* query @ key 得到我想关注我的哪些地方（权重表示）\n",
    "* value 表示我各个位置想提供什么样的信息\n",
    "\n",
    "整合输出的注意信息就是 out = (query(x) @ key(x).T)*value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5b9336b-7e0a-499a-8f8a-e7f814d2f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意头的大小设置\n",
    "head_size = 16\n",
    "# 每个头需要一个 key 矩阵，作用到x上以后提取x的特征信息\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# 每个头需要一个 query 矩阵，作用到x上以后提取想问的问题\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# key,query作用于x得到具体的key和query\n",
    "k = key(x)  # (B,T,16)\n",
    "q = query(x)  # (B,T,16)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# 额外再加一个value矩阵直接对x进行作用\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# v 也直接作用于x\n",
    "v = value(x)  # (B,T,16)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16cb00a1-e6f7-4914-8b11-b427ddccb6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1addb29-a97e-46cf-905a-69e4faf6d2c0",
   "metadata": {},
   "source": [
    "### 3.3 添加系数控制方差\n",
    "在 attention is all your need 的论文中，计算out的时候还会除以 head_size^0.5 这么个系数  \n",
    "原因是为了控制矩阵的方差不因计算而膨胀  \n",
    "举例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3051bf57-d1bc-4fae-bacf-dd1197a542f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81b503d0-f551-4a19-bff2-13185111bb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9847)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.randn生成的都是均值0方差1的随机数，方差期望为1\n",
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9b30832-9f3a-4aec-9a6c-13f40d19d217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1053)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.randn生成的都是均值0方差1的随机数，方差期望为1\n",
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e328cad5-e31b-4fa8-b81b-4508826f7a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.2691)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但计算后的wei的方差的期望却膨胀到了 head_size 倍\n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d625478-ebd3-4d82-806e-75039ee789b0",
   "metadata": {},
   "source": [
    "方差变大的原因如下：\n",
    "* 如果 X,Y随机变量都满足 均值0方差1，可计算 X*Y 也满足均值0方差1  \n",
    "* 由于计算 wei 即k和q的内积的时候相当于是计算了 head_size 组不同 X*Y 的和。  \n",
    "* 求和会导致方差为各随机变量的方差相加，于是方差的期望变为 head_size\n",
    "\n",
    "\n",
    "方差变大会有什么后果：\n",
    "* 而方差增加会导致每一层的权重在方差上不一致\n",
    "* 特别当方差非常大时，对应到softmax函数（scale敏感），会产生尖锐集中的分布\n",
    "* 这容易导致梯度消失，进而导致训练效果受限\n",
    "* 同时我们也不希望每个 token 仅向少量几个其他 token 获取信息（wei的分布不应太尖锐和集中）\n",
    "* 所以需要添加一个系数去控制方差\n",
    "\n",
    "举个例子看softmax的尺度敏感性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32e9c6ed-c4a2-48f4-a0aa-06a73d90385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4427,  0.8926, -0.7062, -0.7538,  1.3745, -0.1691, -1.4154, -0.6329])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个一维张量\n",
    "a = torch.randn(T)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fad9804d-c36a-4169-a767-e5303921f9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0668, 0.2538, 0.0513, 0.0489, 0.4110, 0.0878, 0.0252, 0.0552])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取softmax\n",
    "b = torch.softmax(a, dim=-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0ede379-4df7-4146-858c-796cb499511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2716e-08, 8.0136e-03, 9.1273e-10, 5.6675e-10, 9.9199e-01, 1.9629e-07,\n",
       "        7.5872e-13, 1.8981e-09])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把所有元素扩大10倍以后取softmax\n",
    "# 可以看到往a里面最大地那个值，占有了绝大部分b的结果，即集中度更高了\n",
    "# 这是由于softmax是指数函数作用后的归一，而指数函数是 scale 敏感的\n",
    "b = torch.softmax(a * 10, dim=-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678dbf6-df88-42af-a749-2c252c25798d",
   "metadata": {},
   "source": [
    "由于计算已知方差扩大了 head_size 倍，所以仅需对结果除以 head_size^0.5 则能达到控制方差的目标。  \n",
    "所以修改后的 attention 代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5826fe3-61c5-4e97-a3f7-d182e2b0fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意头的大小设置\n",
    "head_size = 16\n",
    "# 每个头需要一个 key 矩阵，作用到x上以后提取x的特征信息\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# 每个头需要一个 query 矩阵，作用到x上以后提取想问的问题\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# key,query作用于x得到具体的key和query\n",
    "k = key(x)  # (B,T,16)\n",
    "q = query(x)  # (B,T,16)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# 额外再加一个value矩阵直接对x进行作用\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# v 也直接作用于x\n",
    "v = value(x)  # (B,T,16)\n",
    "# 计算注意后的结果，并且乘以系数稳定方差的期望\n",
    "out = wei @ v * head_size**-0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310 for torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
