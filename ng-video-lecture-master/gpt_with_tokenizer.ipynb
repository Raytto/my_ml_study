{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531143d9-2429-484b-ad32-ecbdbcdc60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0acd7-4e16-425c-afe7-de7dee23f91d",
   "metadata": {},
   "source": [
    "## 1. 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93727bdb-ba2d-4a2f-9620-051f3992ece0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24e62e40a30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各种超参，可以之后用到了再反过来看\n",
    "batch_size = 64  # 训练时的并行度\n",
    "block_size = 256  # 每次基于多长的上下文去预测\n",
    "# max_iters = 5000  # 训练次数\n",
    "eval_interval = 200  # 每过多少次进行一次损失评估\n",
    "learning_rate = 3e-4  # 学习率,transformer的学习率不能太高\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # 使用设备最好有cuda,我用的3070ti这个参数规模要训练大概20分钟\n",
    "eval_iters = 200  # 每次评估用多少批数据\n",
    "n_embd = 384  # 每层attention将上下文转化成的向量size的总量(每个注意力头会均分)\n",
    "n_head = 6  # attention head 数量\n",
    "n_layer = 6  # 多少层 attention\n",
    "dropout = 0.2  #\n",
    "# ------------\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a941a-a26c-45e2-a471-60f61ebe7233",
   "metadata": {},
   "source": [
    "## 2. 准备学习数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6342f-c934-4dbf-b401-dd410bf2272d",
   "metadata": {},
   "source": [
    "### 2.1 语料准备\n",
    "我这里用的哈利波特1-7部，下载链接：\n",
    "* https://github.com/LouisScorpio/datamining/blob/master/tensorflow-program/nlp/word2vec/dataset/%E5%93%88%E5%88%A9%E6%B3%A2%E7%89%B91-7%E8%8B%B1%E6%96%87%E5%8E%9F%E7%89%88.txt\n",
    "\n",
    "Andrej 教程用的是莎士比亚文集，没有本质区别，根据喜好选择即可\n",
    "* https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "下载到python同目录，改一下名字即可（'Harry Potter 1-7.txt'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a9bcecf-89f6-48a9-8409-99becdcf926c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Harry Potter 1-7.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m the_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarry Potter 1-7.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mthe_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mansi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Harry Potter 1-7.txt'"
     ]
    }
   ],
   "source": [
    "# 加载语料\n",
    "text = \"\"\n",
    "the_file_path = \"Harry Potter 1-7.txt\"\n",
    "with open(the_file_path, \"r\", encoding=\"ansi\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d902d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录是: D:\\ppfiles\\myprograms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "print(f\"当前工作目录是: {current_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78e4f2d-9ae0-49e9-9f63-acd895a660cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1.Harry Potter and the Sorcerer's Stone.txt\\n\\n\\u3000\\u3000Harry Potter and the Sorcerer's Stone\\n\\u3000\\u3000CHAPTER ONE\\n\\u3000\\u3000THE BOY WHO LIVED\\n\\u3000\\u3000Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\\n\\u3000\\u3000Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\\n\\u3000\\u3000The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they coul\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查一下头1000字符j\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bdc683-bbac-4113-af77-cf5aed752f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查发现文本的换行不统一，有些地方仅一次回车有些地方又两次，修改一下统一为两次回车t\n",
    "placeholder = \"##DOUBLE_NEWLINE##\"\n",
    "text = text.replace(\"\\n\\n\", placeholder)\n",
    "# 将所有单个的'\\n'换为'\\n\\n'\n",
    "text = text.replace(\"\\n\", \"\\n\\n\")\n",
    "# 恢复之前placeholder的成对'\\n\\n'\n",
    "text = text.replace(placeholder, \"\\n\\n\")\n",
    "# 再删除文中的一些全角空格\n",
    "text = text.replace(\"\\u3000\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbac6456-2e9d-4422-b456-146f301b336d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6360384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f8338c-f617-471b-87d5-074ab923f7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1.Harry Potter and the Sorcerer's Stone.txt\\n\\nHarry Potter and the Sorcerer's Stone\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查一下头1000字符j\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc8e3f2-235a-4292-bf9e-e875226afddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查一下后1000字符\n",
    "# text[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c820f-27ff-406b-bbb4-ffac00957496",
   "metadata": {},
   "source": [
    "### 2.2. Tokenization\n",
    "用我们基于 BPE 训练的 Tokenizer 进行编码和后续解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee1002d-46ee-468d-b8eb-d05198618744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中加载模型\n",
    "with open(\"bpe_data.pkl\", \"rb\") as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "merges = loaded_data[\"merges\"]\n",
    "vocab = loaded_data[\"vocab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbf990e-d7d3-48cd-b738-1b98d375faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce51f9d-fde7-45b6-b4d9-e735483ce43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b62acc-47d6-45cd-909c-c55b7a871d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    获取ids中各个pair的出现次数\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]):  # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ca4599-0201-4b24-b11f-37a0413e5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换函数\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    将 ids 序列中的 pair 替换为 idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd084d5-71f1-4fed-8fce-5c311f5adff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由原始文本到token ids\n",
    "def encode(orginal_text, merges):\n",
    "    # 经由原始字符转化为token id\n",
    "    text_bytes = orginal_text.encode(\"utf-8\")  # raw bytes\n",
    "    ids = list(text_bytes)  # list of integers in range 0..255\n",
    "    while len(ids) >= 2:\n",
    "        # 从前往后替换  （和训练时保持一致的先后顺序）\n",
    "        stats = get_stats(ids)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        # 如果没有则返回inf(最不优先替换)\n",
    "        # 但如果全部都是inf的话 min 会返回第一个需要判断下\n",
    "        if pair not in merges:\n",
    "            break  # 替换完成\n",
    "        # 替换最早的那个pair\n",
    "        idx = merges[pair]\n",
    "        ids = merge(ids, pair, idx)\n",
    "        # 监控进度\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"done with id {idx}\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f00f85-8b88-4b12-9bb0-de493f1857c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由token ids 到原始文本\n",
    "def decode(ids, vocab):\n",
    "    # given ids (list of integers), return Python string\n",
    "    text_bytes = b\"\".join(vocab[idx] for idx in ids)\n",
    "    # 如果decode有问题（如utf-8编码下第一个byte不可能是128）则用一个特殊字符代替\n",
    "    text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc78054-766e-497c-807c-8e1a06472a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920090c9-a0f8-4436-ba43-f6e939afff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with id 280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[352, 280, 111]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 试用一下\n",
    "encode(\"hello\", merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb4f4636-c88a-49e2-9505-a5ad437c0882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edeno'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 试用一下\n",
    "decode([335, 277, 111], vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87607a-e254-4a4d-a4ba-ba0b539dc1ac",
   "metadata": {},
   "source": [
    "### 2.3 训练集验证集切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "641ae67c-09b9-4d0e-97a0-36f7072d1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with id 260\n",
      "done with id 270\n",
      "done with id 280\n",
      "done with id 290\n",
      "done with id 300\n",
      "done with id 310\n",
      "done with id 320\n",
      "done with id 330\n",
      "done with id 340\n",
      "done with id 350\n"
     ]
    }
   ],
   "source": [
    "# 先 encode\n",
    "data = torch.tensor(encode(text, merges), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b8de3a-6659-494a-8787-d3ed2faac50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 分割一下，90%作为训练集，10%用于验证集\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc350890-57e8-4f00-b5be-536340506ca1",
   "metadata": {},
   "source": [
    "### 2.4 随机获取数据函数\n",
    "方便后续随机梯度下降训练和验证时拿数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3aca3979-09f4-4202-83c4-c855f0db932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # 区分训练集和测试集\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # 随机batch_size个序列起始index\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # 根据起始位置选出batch_size个block_size长的序列\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    # 往后偏移一位作为y\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    # 数据存在CPU或者GPU上\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f35fe-c471-4652-b84f-788a0bc7c50f",
   "metadata": {},
   "source": [
    "## 3. 搭建简易GPT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fe3f5-3689-4690-96c4-e3ca92c3acad",
   "metadata": {},
   "source": [
    "### 3.1 损失估计函数\n",
    "用于训练中阶段性监测训练情况的损失估计函数  \n",
    "注1：由于是抽样估计，所以仅是估计函数，而非针对 训练集/验证集 的准确损失函数  \n",
    "注2：由于是用于估计损失，所以没比较浪费算力计算梯度，为此可以加一个 @torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140b8880-0f82-4ab9-b11c-331014d28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    # 设置模型为评估模式，通过设置为评估模式，可以确保模型在验证或测试时的行为与训练时保持一致，但去除了训练特有的随机性，从而使评估更加稳定和一致。\n",
    "    # 而训练模式下有些层，如Dropout和Batch Normalization会随时对模型本身进行修改\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # 进行一次预测，model会调用模型的forward()函数\n",
    "            logits, loss = model(X, Y)\n",
    "            # F.cross_entropy() 得到的结果对象还包含很多其他操作和信息，想获得具体的交叉熵的值需要用 loss.item()\n",
    "            losses[k] = loss.item()\n",
    "        # 多次抽样取平均，使评估更加准确\n",
    "        out[split] = losses.mean()\n",
    "    # 回到训练模式\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1fe887-7643-4a08-9009-b6e78beb2fa3",
   "metadata": {},
   "source": [
    "### 3.2 单头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3efeee91-2353-4eea-9b61-fd9032512419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # 每个头需要一个 key 矩阵，作用到输入上以后提取输入的特征信息\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # 每个头需要一个 query 矩阵，作用到输入上以后提取想问的问题\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # 额外再加一个value矩阵直接对输入进行作用，获得输入想提供的信息\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # 用一个下三角矩阵，方便后续mask注意力权重矩阵(使每个位置仅注意前面的token)\n",
    "        # 由于它不作为模型的权重参数，不参与训练，所以需要记为 register_buffer ，避免在训练中变化\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        # 根据<Dropout: A Simple Way to Prevent Neural Networks from Overfitting>\n",
    "        # 训练中随机丢弃一些节点，更可能避免过拟合，也避免模型过于依赖部分节点\n",
    "        # torch 会在推理阶段禁用 dropout 层\n",
    "        # 为了推理阶段和训练阶段权重求和的期望一致，所以在dropout的同时会基于dropout率对结果进行缩放\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 一次向前传播\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        # 每个头需要一个 key 矩阵，作用到x上以后提取x的特征信息\n",
    "        k = self.key(x)  # (B,T,hs)\n",
    "        # 每个头需要一个 query 矩阵，作用到x上以后提取想问的问题\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # 让 k和q进行内积，获得key和query到底有多么匹配(用内积的方式计算匹配度)越匹配越值得被注意\n",
    "        # 为了求内积，需要转置一下后两维 (B,T,hs) @ (B,hs,T) --> (B,T,T)\n",
    "        # 注：忽略B以后，k 和 q 都是由T个行向量组成的矩阵，内积有T*T组，对应T*T结果\n",
    "        # 这个 wei 表征每个位置会多么关心其他各个位置的信息\n",
    "        wei = (\n",
    "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
    "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        # 使每个位置仅注意前面的token，后面信息对前面的位置的贡献置为0（softmax(-inf)==0）\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # 向前传播时也随机丢掉一些节点\n",
    "        # 注：当使用 model.eval() 时，PyTorch 会自动禁用 Dropout\n",
    "        wei = self.dropout(wei)\n",
    "        # 计算每个位置愿意提供的信息\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        # 注意力系数乘以前面各个位置愿意提供的信息，得到每个位置注意到的前面信息\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0650655-71a6-4826-9306-9b33dd698b7a",
   "metadata": {},
   "source": [
    "### 3.2 多头注意力机制\n",
    "并行应用多个单头注意力机制，再把各个头的注意到的信息进行一次交换(通过proj线性层映射)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c1f849-f31e-4322-ab54-a0100647d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # 同时应用多个单头\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # 根据 <Attention Is All Your Need> 要求在注意力结算后，还需要一个线性层进行各个头之间的信息交换\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # 这里同样在训练中随机丢弃一部分节点\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 各自计算各个头的注意结果(彼此相互没有信息交换)， 吧各自注意到的结果拼接在一起\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # 把各个注意头的结果经由 proj 矩阵进行交流\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4260e5d-1d13-412d-a8b1-671b24008808",
   "metadata": {},
   "source": [
    "### 3.3 前馈神经网络\n",
    "需要有非线性层(避免模型等同于一个矩阵、或梯度消失问题)  \n",
    "选择方式是将特征空间变大，以提取更多信息，ReLU后，再映射回原特征空间（保持稳定的特征维度，方便layer堆叠）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e71262f3-ee6f-4598-99fc-37781123d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 映射到高维特征空间\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            # 映射回去\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fc66a-1815-4f13-888e-59e6e635516d",
   "metadata": {},
   "source": [
    "### 3.4 完整 Transformer 块\n",
    "包含以下组件\n",
    "1. 多头自注意力层（Multi-Head Attention）：用于从输入中提取相关性，捕捉全局的信息。\n",
    "2. 残差连接和层归一化（Residual Connection + Layer Normalization）：多头注意力层的输出加上输入，然后进行归一化。残差连接帮助梯度流动，避免梯度消失。\n",
    "3. 前馈神经网络（Feed Forward Network, FFN）：包含两个线性层和一个 ReLU 激活函数，用于在每个位置上对特征进行非线性变换。前馈网络的作用是增加模型的非线性能力。\n",
    "4. 残差连接和层归一化（Residual Connection + Layer Normalization）：前馈网络的输出加上多头注意力的输出，然后再次进行归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a9fb616-a6f3-419e-8a84-cff1d4655297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # 每个头平均分特征嵌入的维度，以保证拼接起来刚好是嵌入的维度\n",
    "        head_size = n_embd // n_head\n",
    "        # 前面定义好的多头注意力机制\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        # 前面定义好的前馈神经网络\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        # 归一化后再对每个元素进行缩放和偏置的层(缩放和偏置的权重可学习)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        # 归一化后再对每个元素进行缩放和偏置的层(缩放和偏置的权重可学习)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 之所以将输入直接加回输出，是为了保证信息流更顺畅，减轻梯度消失或梯度爆炸问题，提高训练效率。\n",
    "        # 或理解为保留一条“高速公路”使梯度能更好地传导。\n",
    "        # 见论文<Deep Residual Learning for Image Recognition>\n",
    "        # 但由于用了加法为避免特征规模越来越大所以需要归一化\n",
    "        # 首先对输入 x 进行层归一化，然后经过多头自注意力层（sa），再与输入 x 相加，形成残差连接\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # 再次对输入进行层归一化，经过前馈神经网络（ffwd），再与输入相加，形成第二次残差连接\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00923f97-98b6-4ea3-b292-35d81ad19122",
   "metadata": {},
   "source": [
    "### 3.5 完整的简易 GPT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d60d0f0-5a20-402a-9409-f6544986cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 每个 token 直接通过查找表读取下一个 token 的 logits\n",
    "        # token 嵌入层，将词汇表中的每个 token 映射到嵌入向量\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # 位置嵌入层，用于表示每个 token 在序列中的位置\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # 多层 Transformer 块堆叠\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        # 最终的层归一化和缩放\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # 映射回词汇表\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # 更好的权重初始化，这部分在原始 Andrej GPT 视频中没有提到，但很重要，Andrej 会在后续视频中介绍\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # 根据ChatGPT的解释，这种初始化方案源于实际的实验经验，操作和效果如下：\n",
    "    # 1. 防止梯度消失或爆炸：使用正态分布初始化权重（均值为 0，标准差为 0.02），可以确保初始权重不至于太大或者太小。\n",
    "    # 2. 更快收敛：初始化的标准差为 0.02 是经验上的一个好的选择，尤其是在 Transformer 模型中，这样的权重分布有助于让模型更快地找到收敛路径。\n",
    "    # 3. 偏置为零：对于偏置，使用零初始化（torch.nn.init.zeros_()）是一种简单有效的方式，它能确保初始时每个神经元的输出都是平等的，不存在偏向。\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # 向前传播x\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx 和 targets 都是形状为 (B, T) 的整数张量\n",
    "        # (B, T, C)，词嵌入\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        # (T, C)，位置嵌入\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        # (B, T, C)，将词嵌入和位置嵌入相加,使x同时具有词性和位置信息\n",
    "        # 注：用加比用cat更好：\n",
    "        # 1. 加不影响维度\n",
    "        # 2. cat其实是特殊的矩阵乘再加的操作。所以对比而言矩阵乘再加更灵活\n",
    "        x = tok_emb + pos_emb\n",
    "        # (B, T, C)，通过堆叠的 Transformer 块\n",
    "        x = self.blocks(x)\n",
    "        # (B, T, C)，最终的层归一化\n",
    "        x = self.ln_f(x)\n",
    "        # (B, T, vocab_size)，通过线性层得到下一个 token 的 logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # 区分学习和纯推理，纯推理则不需要计算损失函数h\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # 再次获取其三维 batch_size*block_size*vocab_size\n",
    "            B, T, C = logits.shape\n",
    "            # 把前两维展开成一维\n",
    "            logits = logits.view(B * T, C)\n",
    "            # 目标值也展开成一维\n",
    "            targets = targets.view(B * T)\n",
    "            # 计算交叉熵以估计预测的概率分布和实际值的差异（cross_entropy支持一个是概率分布，一个是目标值，这种两个类型和维度的输入）\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # 生成下一个token预测\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # 循环生成 max_new_tokens 次\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 将 idx 裁剪为最后 block_size 个 tokens数量不足的话用0填充(0在前面的编码中对应回车)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # 拿到每一位对下一位的预测结果 logits，也对应 batch_size * block_size * vocab_size 的张量\n",
    "            logits, loss = self(idx_cond)\n",
    "            # 其他抛掉，只看最后一位\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # 用softmax函数作用（变到[0,1]区间，且和为1），作为预测的各个后继的概率。dim=-1表示沿最后一个维度进行作用\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # 对每个行，根据概率进行抽样得到序号\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # 把当前的抽样结果加在序列后面\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1af250-26e7-4efb-89a8-ee7dddd398ec",
   "metadata": {},
   "source": [
    "## 4. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ba34711-5a6a-4724-849b-1bd068e6d1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.015784 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# 计算并显示参数总量\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0728a3c7-2a3f-4670-8899-7058a078cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建优化器，使用了 AdamW（可以动态调整学习率，提升收敛效率）\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aa33f5b-c5b3-46b3-b3d8-a6cdfc503248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_times):\n",
    "    for iter in range(train_times):\n",
    "\n",
    "        # 到损失检测的时候，进行一次损失估计，方便监控收敛情况q\n",
    "        if iter % eval_interval == 0 or iter == train_times - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # 取一批数据\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        # 计算损失函数\n",
    "        logits, loss = model(xb, yb)\n",
    "        # 将梯度置为0，如果不置0则梯度会累加。因为 PyTorch 在默认情况下对梯度进行累加，以便在某些情况下可以手动进行累积梯度（例如在处理大批量数据时分批计算）\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # torch 自带的反向传播计算各参数梯度\n",
    "        loss.backward()\n",
    "        # optimizer 对各参数基于其对于损失函数的梯度进行一次更新，更新的step具体值由梯度和优化算法共同决定，如这里的优化算法 AdamW\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44b361af-3e38-4240-8979-c7a8d09b6564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ed ���owęed_reoral�owgh\u0005earo-one ��uar�K�l\u0012ow\u0010A\u000fyou H\u0012 or)\u000e�it�it�hanothe )nHarrll*ly ����B,le riarere �\u0005�in T���, �R�ing d�\u0015lowP��k �ve  s�/�I��er����\u0003Bere \" Ї�one �ed ��aid �sve oHarr\u000firw_'��al9ostIaive >�r�s \t sTe, re�os you acof ai�th was �a ione you\u0015v�the�he \u0000<vc�the��;sslesing un( said �O�inIs to \u0003King`f��en ސ. HarrarrHen �\u001b\n",
      "\n",
      "uer �cRghgޑ�\f\n",
      "thlithe �oo�sa t1���l)noGdk?���Rer0]��Y�Harry 0s�k \u0012ghn�l�l��jhad rie%eous 7�~_ves\u0006icwas )\n",
      "er a0�b��roar�in Harr�\u0018rok �b\u001f\u0014ve riV's ��ea�the �<eapt thehe ter�you �oohaPle�y, \u0016ri9'hiuy, -gӛat�\u0003foweawa~ing#\u0003the <\u0018�=, �ss berdd t @rto e, `algh-e, ��r\u0015k ��0i���noo}ou\u0015#�o�C\u000f�We=of �?wa�.\tit ��Harry �C��ve �����mto \u0000Harrto �����vchlicb\u0012ing�ac�a \u0013�l�vat\u0013he he ��;\" o Qed �ouenur���orne �o Only \u0015�ai��u��\u0000ri'�� oo�he �4.to skwas .\n",
      "\n",
      "�_ڄv�l���\u0002�ou.ea8aid Rouea ��it it ]!�0acha�d�\u001ak �reood�\n",
      "�y��\\��\u0012�:cove }���-?u�k \u000fk to ��ll �L s�or AZbe��9�\u0012�an:�. wiy, q�that ree bb��thehehis �\u0014er�4!�~��noac<�ip+to ing haHarr�ai�mi�ir� 1ly \u0003~y, on ��#�er the (�one aid ��. �<�.\n",
      "\n",
      "of �in b�stb\u0013s where [�lehatHarr\u001ao�DA�the �\u0010octhe A�$Is ar\u000e�d�mipalhe+sk of ;P'Z s��<oHarrt ron �&���ging he�mijle�.\n",
      "\n",
      "y, ��~. it '��\u0004oo׼ehreur�-er ��icfo@haor)�onur��Wchicainoll�BHarry �XT0s old �]6�\u0015\t巈�e m and ��anZ�acliar,!�o�ld �ing \u0001Us \u0001ou�\u000f�4~y, �e�����arb�S�onno��:ed �FM�ing �Be6at inn�e 's �>st\u0015and \"noere @arcH�,\u0010owwaom�\u001fPm \\�+mthat <}Oirare��ll\"�\u000f��=in�ed e the e itou\u0012arleMve ��B�b�t\u0011�ly �֋agt8er-�W\u0015at lele q��\u0007wy, t\n",
      "*ri��aid �you ouI �itlat �?+D's donou�ve th�at �bs\"�no�no'V�\u001b�d \n"
     ]
    }
   ],
   "source": [
    "# 先试试未训练时的生成效果\n",
    "# 以 'I ' 开头\n",
    "context = torch.zeros((1, 2), dtype=torch.long, device=device)\n",
    "context[0][0] = encode(\"I\", merges)[0]\n",
    "context[0][1] = encode(\" \", merges)[0]\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist(), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1135a984-c296-40af-9195-228f9830613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.3148, val loss 5.3151\n",
      "step 200: train loss 3.3250, val loss 3.3436\n",
      "step 400: train loss 2.6803, val loss 2.7322\n",
      "step 600: train loss 2.3169, val loss 2.3941\n",
      "step 800: train loss 2.1576, val loss 2.2524\n",
      "step 999: train loss 2.0564, val loss 2.1648\n"
     ]
    }
   ],
   "source": [
    "# 先训练 1000 次\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a5e2634-510c-4e31-86e8-c25fcca72f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I exuse, unafter a march to call of voice, and his teaching higher, ruding over the stairs, it dark ted between his head.\n",
      "\n",
      "    'CH\n",
      "\n",
      "EH Wood,' Ernies stonct to \n",
      "\n",
      "pointing and calling the school as he walking econdition. Sides from the dementors against air out of ears around them at him. If the page, the maze better Vastle, or Dalps of them was sister of archment could returning in himself into compart ase. Evoise miss there, search tack in, so Harry when sonal passed in the care of the corner, not all her\n",
      "\n",
      "they lower into intere, in underc\"suaes a low and they fingeriuntom! Homauld forled 'eop a Serucous on them! When they'�mist?\" and seized.\n",
      "\n",
      "\"All riNis!\" asked Ginny, rapping breathing wildarble. \"Priving their seeth, too's tongue off how madered? A most unHagrid \n",
      "\n",
      "all around the \n",
      "\n",
      "light and recognar as Harry walkd-Ey, Lily Patronague–\" \n",
      "\n",
      "\"Hella!,\" he said. \"There Pervan in the room. . . . . .” \n",
      "\n",
      " “Ere it is cleandable time I get, first pip you in a little foist,” she added, watrans. Hagrid's crimpasses, Harry looked at the coll and down on his got table. “Lupin tom! Well — ' \n",
      "\n",
      " To Brook Ron,” was shorrowing them a familiar \n",
      "\n",
      "“I naught you drinins!' know the deze, there was a card.\n",
      "\n",
      "“What – he’re sup the secontrat ront s in Briday, disapprudy!” \n",
      "\n",
      " She binky: Hagrid, seized Snape \n",
      "\n",
      "it’s impkonse throom in front racing \n",
      "\n",
      "the Daily an Harry, ParvenEmple. Harry sawed friender frog, and so at Hagrid’s appeared, Harry gadthugh interin to water. Justin and swallowed the \n",
      "\n",
      "treepers in his toald, and they all through Harry to kept you seize Ron and Ginny. “I’ve had got try to go to the wind reach position of despossi-Mayle whole right. \n",
      "\n",
      " “Ha!” \n",
      "\n",
      " “Beolboard,” he said quietl\n"
     ]
    }
   ],
   "source": [
    "# 生成一下看看情况\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist(), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99caac05-54ee-4385-91f5-05e1ea9f48fb",
   "metadata": {},
   "source": [
    "注：生成的质量进步明显，且 loss 看起来并没有收敛，有继续训练的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f606b0d-8101-4c92-a312-8c048a5a5395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.0588, val loss 2.1646\n",
      "step 200: train loss 1.9920, val loss 2.1198\n",
      "step 400: train loss 1.9374, val loss 2.0776\n",
      "step 600: train loss 1.8985, val loss 2.0346\n",
      "step 800: train loss 1.8627, val loss 2.0159\n",
      "step 999: train loss 1.8339, val loss 1.9861\n"
     ]
    }
   ],
   "source": [
    "# 训练 1001-2000 次\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b20c4aed-c828-4595-9b57-87ff68db2313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I up the evening,” \n",
      "\n",
      "“It doing,” said Harry. \n",
      "\n",
      " Mrs Diddle’s piece of cage to searcase, didn’t. It had deliqustimided netension of obvious, he feeled that he said it was sent \n",
      "\n",
      "towards the carryer, right and swall to make himself in \n",
      "\n",
      "Breath and a \n",
      "\n",
      "luncht disappearance of the torches. He heard a Gaunt forating not plain. It, it alone. \n",
      "\n",
      "“But for a bombaby. To know hold it.” \n",
      "\n",
      "“You left,” said Hermione, still made her way with him as on the \n",
      "\n",
      "passage, chetting the Daily Prophet had reached the \n",
      "\n",
      "madoor, Hutching marlie purple, and a spellbook, whog, and trief opened in\n",
      "\n",
      "ead. All of continued attrippers, tiny \n",
      "\n",
      "y dressed his headmistress? \n",
      "\n",
      " “. . .” \n",
      "\n",
      "Ron whispered the haunnot els. Harry p\" Moody examined through the back of the tightmare being and talk to Hermione. Whether Harry had explainly set that \n",
      "\n",
      "coolly’s rumphed away front of it, beaming upon Mr. Cattered for a hamper sap \n",
      "\n",
      "her beak-robes to brightly fent.\n",
      "\n",
      " “Vernon merops, glin and Deanwhile you ought to incredular stress with the palner of Harry and gockase, whatt, and \n",
      "\n",
      "was thickbened that she got for excited to such thescene course, in a hushed voicementc! The Ministry without a with a blap houlder of the wand in SHarry and Dursleys, told them in this less, and that \n",
      "\n",
      "was Crubbunzarr, and they would say hear it habout him to find out an end him being unplaced in his navement many career back normal as hadopp. And I haven't seem to lad how him anything happened on Schools... Why was half.\n",
      "\n",
      "This one rained me, who went on, Mrs. Durrady the \n",
      "\n",
      "sink was most me. They \n",
      "\n",
      "stared for a moment, already look. They rare just before they was Runnows starting to be the traveloping \n",
      "\n",
      "vagulously burn into an across %; \n",
      "\n",
      "Hagrid's normalt left and into the bow om began to limble of coin, removed vi\n"
     ]
    }
   ],
   "source": [
    "# 再生成一下看看情况\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist(), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361597f-4546-49e4-8acc-78fb59178833",
   "metadata": {},
   "source": [
    "注：似乎质量确实又高了一些，且val loss看起来还在稳步降低，再训练 1000 次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4aa4c73f-2bf7-4062-9655-d0bea3fc1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8371, val loss 1.9926\n",
      "step 200: train loss 1.8057, val loss 1.9743\n",
      "step 400: train loss 1.7830, val loss 1.9576\n",
      "step 600: train loss 1.7592, val loss 1.9386\n",
      "step 800: train loss 1.7442, val loss 1.9324\n",
      "step 999: train loss 1.7273, val loss 1.9188\n"
     ]
    }
   ],
   "source": [
    "# 训练 2001-3000 次\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cd9cbf6-09fc-4db3-a0ea-87133f6b53c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      "\n",
      " \"He might think you're more,\" came Madame Aunt Hermione hard and remazing onhot outside. \"He want’ve \n",
      "\n",
      "ved and come back there up, thisn't it? Why’re \n",
      "\n",
      "seeking from – over to be worthy? Harry – how he’s rat? His not asked a Horntail of twenty \n",
      "\n",
      "hidden new settles. He had none of thing that it had won’t go to.\" \n",
      "\n",
      "\"I'd have told \n",
      "\n",
      "–\" he said. \n",
      "\n",
      "Harry stared around at the last of the mountain newsping room and Luna \n",
      "\n",
      "Sorting to stop outside her. The dess was approached there, he shook his hand. \n",
      "\n",
      "\"I aware like this!\" said Hermione in igonal from the first red to Spect \n",
      "\n",
      "heirst singdule at her. \"Er ?he supposed ter me.\" \n",
      "\n",
      "Harry tried about time to say \n",
      "\n",
      "it; he had seen another in shiny, silence behind him. Ron large in the bill's nose —hunfoy, gave him as he couple he pushed at once, thought she was still still disapping \n",
      "\n",
      "the copy of the Hogword to Ron.\n",
      "\n",
      "\"It found how when they're we're Dumbledore of second knew that the followed \n",
      "\n",
      "downward continuers contribut after a spat was waken something that night. \n",
      "\n",
      "\"Ata mounter-of-- a row tuneerly obey-two claim while asleepished. I hope these wanders –\" \n",
      "\n",
      "\"Have a run with the Dark of emerald — �ever,\" Morfin\n",
      "\n",
      "Harry students of Ron hastily suddents in their ownersides of navement mesely's four-matters. He head to a justice toward the lock-\n",
      "\n",
      "PAPeakes Pomfrey and (Malfoy and Hermione told him he'd better go upright, then prepecto a loak, and pleased its wandlying on her) were thugh he'd something he climbed the boggart green him.\n",
      "\n",
      "\"No it's my Goster!\" Hermione hispered.\n",
      "\n",
      "\"It was say,\" said Hagrid. \"Did they could go need started out with him to be pretturn - not there! I can't see us neither.\"\n",
      "\n",
      "They walked cauldron.\n",
      "\n",
      "2\n",
      "\n",
      "\"If see \n"
     ]
    }
   ],
   "source": [
    "# 再生成一下看看情况\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist(), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc8a93-0427-41f0-9b0c-a5edf8605e76",
   "metadata": {},
   "source": [
    "注：train loss 和 val loss，还没啥太大差距（不太过拟合），再训练 1000 次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3c97161-ec86-492c-94ff-004606eeb04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.7256, val loss 1.9212\n",
      "step 200: train loss 1.7087, val loss 1.9083\n",
      "step 400: train loss 1.6950, val loss 1.8981\n",
      "step 600: train loss 1.6771, val loss 1.8904\n",
      "step 800: train loss 1.6688, val loss 1.8873\n",
      "step 999: train loss 1.6540, val loss 1.8849\n"
     ]
    }
   ],
   "source": [
    "# 训练 3001-4000 次\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3f08f0c-5373-481b-bff2-9ee180bd6305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nextsemblood if it came into Umbridge turning upon him, he \n",
      "\n",
      "resisted a spectacular damplike broken motion that had been. \n",
      "\n",
      " \"I see you,\" said Hermione, and the rain leapt from the lace. \n",
      "\n",
      " \"Yours ago,\" said Harry, \"I want another handwrit\n",
      "\n",
      "life. I met a wand wizardship that lie together, is early Harry' \n",
      "\n",
      "Man ran a load. Harry, young wife was a werewolst, and Winky  sought, but because she was \n",
      "\n",
      "stating in us, fast and Davies is not wear of the post moves, Harry remembers of my escalm, but inmit himself and \n",
      "\n",
      "quiet not idea what Morfin might have felt beyone: the warts stood opening there is laughing and todaybe .... \n",
      "\n",
      "Our fixy Madam Rookwoods were \n",
      "\n",
      "mistaken. Be realized, just put a chamber in furious to move ago. . . .” \n",
      "\n",
      " “Shaa,” said Harry infliently. \n",
      "\n",
      "“Ronan sent you reminion, off,” she said rowly, snatching the pudiel of Ministers. “Says I’ve been \n",
      "\n",
      "what, finally ing, Anithur, Seven to Mafe mattergroitself … \n",
      "\n",
      " ” “Hagrid ifteen, you’re put out his wand up? Verish humm! It too worky a sense!” he shouted at Harry to clash angle of ink away, behind the backs a numbrelltion in your \n",
      "\n",
      "right black-lastic crack and deep close, … “ \n",
      "\n",
      " “I will find find out house this summer!” snore of white: \"Please, I’ve been easy to Dad in the castle. I never four Brizar The Quibblered the Hengoess \n",
      "\n",
      "To Muggle, back draw, the most wishing he would got go and regreen better,” Harry and Oluded \n",
      "\n",
      "Ginny, was extravely resulted. \n",
      "\n",
      " The boy was facing the entrance to penaur apart from like Dumbledore— his own wand which should her school something: all of them joined a gnome, got away in main, \n",
      "\n",
      "as though happening to \n",
      "\n",
      "Hermione’s all mable, which h\n"
     ]
    }
   ],
   "source": [
    "# 再生成一下看看情况\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist(), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf02de-0345-4181-854a-d7dafb6719ef",
   "metadata": {},
   "source": [
    "注：train loss 还在每200次0.01减少，单val loss仅减少0.003，略有过拟合的风险了。就此停止吧。 如果训练数据足够多的话应该能让训练集和验证集的表现更接近一些，能支持更进一步训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d5518-9c9d-4771-8a72-a2fcbb4d5ffa",
   "metadata": {},
   "source": [
    "## 5. 模型参数保存或加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17ef6462-2636-4979-8836-5481cd197476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(model.state_dict(), \"model_params_harrypotter_tokenized.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f141b87-6606-4507-9c55-875cafcfc106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载保存的参数c\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# 加载已保存的参数\n",
    "m.load_state_dict(\n",
    "    torch.load(\"model_params_harrypotter_tokenized.pth\", weights_only=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15f5b8-3447-4e16-872f-04bb9162c81b",
   "metadata": {},
   "source": [
    "## 6. 用 GPT 模型生成后文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee955e3b-be8f-410d-abc1-b3b76b6263a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[357, 274]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"who \", merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d677db7-3684-4338-bae4-53f984d7ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who was coming to him ready. Her arms she slopped into her Slytherins:\n",
      "\n",
      "Mr. But one leapt and that jerkier were explocoed by an antidote spell, which conversation was very continually abandon endles. Mrs. Weasley had planted quiberin through the door hearing her head and Harry thought the Person her advancing subpening left of Fudge. Even Bertha's happy Dumbledore and Hagrill, Mr Weasley’s name interested, that they were cas a short way night remember than Dumbledore. Famous and he was, whose spellbooks, \n",
      "\n",
      "finally little would have kidnailed be weidering like sense weapon? The door of everybody because Harry under the teach for the World Cup! Anyway, he \n",
      "\n",
      "missively. He lost every lucky, and saw that creaking fond of seats? A floor of Oce wizard's crackling into Hagrid grip again and \n",
      "\n",
      "The spiralum had shed and with their brains. The poor wizards around the last for Madam Edawn earer, looking on the way through the sky.\n",
      "\n",
      "Which affle away a dragon empt, within the tiny air, Harry had passed a great door that encountered they'd have been one \n",
      "\n",
      "two when they were urrounded by a subject. Voldest less edup. Sirius.” said Hermione at once, morning the same reflection to the pitch. Harry had been buying woods: \n",
      "\n",
      "He turned right nice hardly by Mosten, in which he was, however, all of Cold, Minister Beauxbatons were insidently to be every obviour, and and the small port of bite bird History of Magical \n",
      "\n",
      "Jerkins wizarding Lestruck kind of day. Aunt Muze that the arrived hair on the between Percy which herogs, resister couldn’t sit with \n",
      "\n",
      "their week-eyed live long boyss. \n",
      "\n",
      "\f\n",
      "\n",
      "\n",
      " “I don’t see this is taluse,” said Sirius, “and life mae with one old evidenners \n",
      "\n",
      "of minute. Persy Parsmellie . . but it had – over those six most of minutes on us in the world \n",
      "\n",
      "poisonal providers. I have neverticed what it happened to the contentness of Hogwarts is valualiate … “ \n",
      "\n",
      " \"Fancy, sir!\" yelled Hagrid. \"Too only to the Dursleys! Okay – for everythin' are dein' woman subeetrated up with a maint yer keep \n",
      "\n",
      "that out to what would have intrips after me with their cozy drawing backs, right she was not daring' and X raint is no - '\n",
      "\n",
      "    A goyle given own bedroom and a tiny chuckle, Nearly Headless Born. And Sarie\u000fhe tolete body watch or excited, his braster bunged inside his head.\n",
      "\n",
      "    He said, pulling his fingers tight together the bleeding into the term three door and shoved the clickles into the door. Barely they counted the committeeth at James Privet Drive, which found them vere.\n",
      "\n",
      "    'Potter, Mrs OUCH!' bellowed. 'We've ready to your OWL See S POFPEC Magic Weasley,' he Diggle warning suddenly 'we'll block Hagrid instrucords jus' as more ever. His heard Mundungus seemed to be quietoo. Over Ron's powers along in front of the top of his chair with into all the way to stop him, snobbing out of the Grimmauld Plook. Harry, Ron, and Head of the firsty finished him pulled his strings right over now, as Perpens swirlted all surface of the Death Eater at Hogwarts. There were strong the worst of a Chocolord...\n",
      "\n",
      "    'Wait a Hall of Restum!' said Harry blandly.\n",
      "\n",
      "    A shoulder. However, Mr that moment did he produce the place where the first Quill were a strange Hogwarts Christmas Shopper, coupled, Harry heard that Hermione's bright glinted blurred suk, his former and she was thought bound, just in time, feeling some sort of like himself. When Harry, who did stop rung Mr Weasley approach the cool\n"
     ]
    }
   ],
   "source": [
    "# 尝试生成一段长点的\n",
    "# 以 'Who ' 开头生成\n",
    "context = torch.zeros((1, 2), dtype=torch.long, device=device)\n",
    "context[0][0] = 357\n",
    "context[0][1] = 274\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist(), vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310 for torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
