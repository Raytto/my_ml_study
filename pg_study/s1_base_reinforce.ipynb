{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.包导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.环境尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02601002, -0.18026812,  0.0195548 ,  0.27624914], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "env.step(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.定义策略网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义策略网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)   # 隐藏层\n",
    "        self.fc2 = nn.Linear(128, output_dim)  # 输出动作概率\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(self.device) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5099, 0.4901], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23283/2038754939.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "# 网络测试\n",
    "\n",
    "policy_net = PolicyNetwork(4, 2)\n",
    "# 将输入数据转换为 tensor\n",
    "data = [0.1, 0.2, 0.3, 0.4]\n",
    "input_tensor = torch.tensor(data, dtype=torch.float32)  # 增加批次维度, shape: (1, 4)\n",
    "print(policy_net(input_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5207, 0.4793], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 网络测试\n",
    "policy_net = PolicyNetwork(4, 2)\n",
    "obs = env.reset()[0]\n",
    "print(policy_net(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.计算折扣奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算折扣奖励\n",
    "def compute_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.设置环境和超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置环境和超参数\n",
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]  # 输入维度\n",
    "output_dim = env.action_space.n             # 输出维度\n",
    "policy_net = PolicyNetwork(input_dim, output_dim)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "gamma = 0.99  # 折扣因子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23283/2038754939.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "num_episodes = 1\n",
    "for episode in range(num_episodes):\n",
    "    # state = torch.tensor(env.reset()[0], dtype=torch.float32).to(policy_net.device)\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    state = env.reset()[0]\n",
    "    # 采样轨迹\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # state = torch.tensor(np.array(state), dtype=torch.float32)  # Ensure state tensor is on the correct device\n",
    "        probs = policy_net(state)\n",
    "        m = Categorical(probs) # 创建一个类别分布\n",
    "        action = m.sample()  # 采样动作\n",
    "        log_probs.append(m.log_prob(action))  # 记录 log(prob)\n",
    "        # 执行动作\n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(policy_net.device)\n",
    "        # print(f\"state: {state}\")\n",
    "        done = bool(terminated) or bool(truncated)  # Ensure done is a boolean value\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    \n",
    "    # 计算累计回报\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(policy_net.device)  # Ensure returns tensor is on the correct device\n",
    "    \n",
    "    # # 标准化回报，以减少方差\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "    \n",
    "    # # 计算损失\n",
    "    # policy_loss = []\n",
    "    # for log_prob, G in zip(log_probs, returns):\n",
    "    #     policy_loss.append(-log_prob * G)  # REINFORCE的损失：-log_prob * G\n",
    "    # policy_loss = torch.cat(policy_loss).sum()\n",
    "    \n",
    "    # # 反向传播和优化\n",
    "    # optimizer.zero_grad()\n",
    "    # policy_loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    # # 输出训练进度\n",
    "    # if episode % 50 == 0:\n",
    "    #     print(f\"Episode {episode}, Total Reward: {sum(rewards)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
