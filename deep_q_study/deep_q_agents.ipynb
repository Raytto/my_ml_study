{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "n_games = 1000\n",
    "win_pct = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(range(env.action_space.n)))\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_games):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        done = done or truncated  # Ensure the loop exits if the episode is truncated\n",
    "    scores.append(score)\n",
    "    if i % 10 == 0:\n",
    "        average = np.mean(scores[-10:])\n",
    "        win_pct.append(average)\n",
    "\n",
    "plt.plot(win_pct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(list(range(env.action_space.n)))\n",
    "print(env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s, a; \\theta) = \\mathbb{E} \\left[ r_t + \\gamma \\max_{a'} Q(s', a'; \\theta') \\middle| s, a \\right]$\n",
    "\n",
    "* Q 是状态 s 和动作 a 下的 Q 值，参数 θ 表示 Q 网络的参数。\n",
    "* $r_t$ 是当前步的奖励。\n",
    "* $\\gamma$ 是折扣因子，让未来的收益打折\n",
    "* $max_{a'} Q$ 是下一步状态 $s'$中最大的 Q 值，$a'$ 是 $s'$ 中所有动作\n",
    "\n",
    "Q 度量的是当前状态和动下下，且未来总用最优动作带来的回报折现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习更新的方法\n",
    "\n",
    "$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, n_actions, n_states, eps_start, eps_end, eps_dec):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma \n",
    "        self.n_actions = n_actions \n",
    "        self.n_states = n_states\n",
    "        self.eps = eps_start\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.Q = {}\n",
    "\n",
    "        self.init_Q()\n",
    "\n",
    "    def init_Q(self):\n",
    "        for state in range(self.n_states):\n",
    "            for action in range(self.n_actions):\n",
    "                self.Q[(state, action)] = 0.0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.eps:\n",
    "            action = np.random.choice([i for i in range(self.n_actions)])\n",
    "        else:\n",
    "            actions = np.array([self.Q[(state, a)] for a in range(self.n_actions)])\n",
    "            action = np.argmax(actions)\n",
    "        return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.eps = self.eps - self.eps_dec if self.eps > self.eps_min else self.eps_min\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        actions = np.array([self.Q[(state_, a)] for a in range(self.n_actions)])\n",
    "        a_max = np.argmax(actions)\n",
    "        self.Q[(state, action)] += self.lr * (reward + self.gamma * self.Q[(state_, a_max)] - self.Q[(state, action)])\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "agent = Agent(lr=0.001, gamma=0.9, n_actions=env.action_space.n, n_states=env.observation_space.n, eps_start=1.0, eps_end=0.01, eps_dec=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
